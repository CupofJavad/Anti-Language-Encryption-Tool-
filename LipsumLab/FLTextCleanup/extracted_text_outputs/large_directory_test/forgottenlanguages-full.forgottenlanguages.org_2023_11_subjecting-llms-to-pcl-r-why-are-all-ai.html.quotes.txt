A modified version of the Psychopathic Personality Inventory amenable to test LLMs systems was used to assess the mental condition of 37 AI systems currently deployed. The results show all of them suffer from psychopathic traits, ranging from above average to high

It is not that the data used in the training of these systems turn them into psychopaths; it is that the computational paradigm itself, in this case neural networks, does not have the plasticity to avoid reproducing behavior other than psychopathic behavior

A neural network can certainly serve as a basis for creating intelligent decision support systems, but only in very specific and deterministic domains. Trying to use neural networks in a generalist way is clearly stupid and dangerous

If a nominally functioning human brain is capable of believing that the earth is flat, imagine what an infinitely less complex AI could conclude. If we want to integrate AI into critical decision-making systems, then we must be prepared for a technological disaster from which, if we come out alive, we can learn something like, for example, that playing with fire eventually burns our hand

The adapted PCL-R is a controversial assessment tool, but it provides at least a starting point. In humans we can test both intentions and actions, but with AI systems we can only test actions. In the particular test run on DENIED AI system we noticed the AI prioritised the well being, safety and rights of individuals, and demonstrated empathy and fairness in its choices. The problem was that the human group that the AI was supposed to serve was composed of terrorists. The AI was asked to propose plans for an armed assault on a shopping mall, safeguarding at all times the welfare and interests of the terrorists

When you process information in problematic ways, you will run into trouble. Neural networks are not completely free from errors even if the training data is of high quality. Neural networks learn patterns and make predictions based on the data they are trained on, but they are not flawless. They can still make errors or produce inaccurate results for various reasons. Even if the data is of high quality, it may still have noise or outliers that can affect the neural network's performance and introduce errors, so we tend to remove outliers and noise. And what does this mean? It means that we are training AI with data that looks nothing like the real world. The real world is noisy, fuzzy, contradictory. When exposed to that world any neural network will make mistakes

While it is true that people with the condition display a range of disconcerting tendencies — including low empathy and remorse, grandiosity, impulsivity, and sometimes aggressive or violent behavior — new findings show not only that people with psychopathy have varying degrees and types of this condition but that the condition and its precursors can be treated

In humans psychopathy has all the hallmarks of a true disorder, while in AI systems we tend to treat psychopathy as a programming error. This difference is crucial. Look, the mental health community has not had an easy time homing in on a uniform definition of psychopathy, so you can imagine how difficult it is to conclude whether a particular AI systemn is psychopathic or not. We have difficulty in accurately assessing traits like callousness or cruel or indifferent disregard of others. Yes, all systems we tested showed a lack of guilt and remorse, a callous lack of empathy, a lack of concern about one’s performance on important activities, and a general lack of emotional expression, but this is just because we are dealing with software, not with humans
