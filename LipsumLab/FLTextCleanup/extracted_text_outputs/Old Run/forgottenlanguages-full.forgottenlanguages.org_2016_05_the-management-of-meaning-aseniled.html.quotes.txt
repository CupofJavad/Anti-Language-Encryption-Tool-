For what concerns the events we are about to experience, disseminating false information is not a valid strategy anymore. We need to provide means for people to infer something that is actually false. We need to tweak the meaning of words

information provided is true and accurate; however, meaning management strategies have worked in such a way as to reach the desired output: people beliefs are changed without we having to misinform, disinform, or lie

whether a person will disinform depends on the expected costs and benefits. In particular, it depends on the costs of not being believed (weighted by the probability that this will happen) as compared with the benefits of being believed (weighted by the probability that this will happen). Thus, there will be a lot of disinformation if the benefits of being believed are high relative to the costs of not being believed and/or if the intended audience of the disinformation is much more likely to be credulous than to be skeptical

those objects do not crash in remote areas in a desert; actually, we down them when they fly over deserted and remote areas. This explains why you find a similar pattern when you analyze crash sites. We know one day an accident could happen and one of those objects will crash amid a densely populated area. What are we going to tell the people? In order to conceal the fact that we were running experiments to study the effects of a nuclear blast over the population, we conducted the Green Run Experiment. What were we supposed to tell our own government? That we were exposing Americans to hazardous radiation levels just to know what would happen in case a nuclear detonation occurs? We told them about Mogul, and nobody realized the sensors in the balloon could only sense mid-range
radiation levels, the one you should expect from radiation being released from the Hanford Site

sadly, this time the joke is on us. When we come into direct physical contact with them, it will be either them, or us
