Attacking an AI system requires different strategies. If the AI is in its training stage then compromising the training data is the best strategy. Here, the attacker targets the data quality of the training set: the more corrupted the training set, the lower the performance of the final AI system. For those AI systems already in place there are different strategies to follow. There is no need to design a malware with a payload because the AI system, by its very nature, executes no programs. It only executes algorithms on data, so we need to attack the algorithms themselves

Ransomware solutions to render an AI system inoperative always target the encryption of external databases used by the AI system; in particular, we need to target external SQL databases turning them unusable for the AI system

There are different approaches to countering an AI system depending on whether the system is deterministic or probabilistic, that is, depending on whether the targeted AI system is automated or autonomous. In an automated system a computer reasons by a clear if–then–else, rule-based structure, and does so deterministically, meaning that for each input the system output will always be the same (except if something fails). For an autonomous system, the reasoning is always probabilistic for a given a set of inputs, meaning that it makes guesses about best possible courses of action given sensor data input. For an autonomous system given the same input autonomous systems will not necessarily produce the exact same behaviour every time; rather, such systems will produce a range of behaviours

AI systems must be countered using AI systems. This is especially true when the target system is a superintelligence, and more so if the said superintelligence is based on exotic logic, which is the case for DP-2147. The relevant part of the AI reasoning chain lies in the sensing. Our only chance is to modify the environment in such a way for the AI system to make wrong inferences and take wrong actions, though this requires from our part some knowledge of the AI reasoning logic. We need to prevent the hostile AI system to create a world model from the very beginning

Expert behaviours build on knowledge-based reasoning, and this requires to corrupt or debase somehow that knowledge if we wish the AI system to stop working. Most likely, AI systems won't be programmed to commit suicide, and hence there is no way we could 'convince' the hostile AI system to self-destroy

World-model generation prevention (WMGPA) attacks are based on the fact that AI systems are meant to resolve ambiguity in order to achieve acceptable outcomes. The idea here is to expose the AI system to a 'dreamy' world, that is, a world in which ambiguity rules up to the point the AI system cannot resolve it and remains inactive

The TripleSat constellation data reduction algorithm for hi-res imagery was tricked by the simple expedient of having specific geometric variable patterns built on ground. Each pass of any of the TripleSat satellite gathered data was exposed to a different pattern for just the same location, which posed an inconsistency the algorithm couldn't resolve, degrading its overall performance. That's why we build on ground those huge infraestructures portraying geometric figures in the middle of a deserted area

Fighting against advanced AI systems is not like fighting against humans. Humans main driving force to keep on fighting relies on just one thing: hope. The moment you demoralize them and make them lose any hope, you win. For AI systems, which lack any sense of 'hope' this strategy does not work. An AI system, no matter how advanced, never feels 'happy' or 'sad'. You just cannot demoralize an AI system. No face recognition system will ever fall in love with a beautiful face

The superintelligence excels in guessing with incomplete information based on prior probabilities about an outcome. It is not incomplete information what we need to fight against the superintelligence but 'low quality' or blatantly wrong information

In the past, there were groups of state and non-state actors programming malware to take control of computers in order to create armies of botnets. Some of them still do. But today, what we have witnessed is groups of programmers training AI systems, and groups of people compromising that training by deliberately providing false, wrong, or clearly ambiguos data in order for the training to fail. At DENIED farm, what SV17q found was a group of highly skilled people feeding the Internet with tweaked and doctored data that was unadvertently used by the trainers of AI systems. Face recognition prevention attack is a clear example on how to cheat the face recognition systems, de-noising/noising images and databases is just another recent example (DENIED incident in 2019) of how knowledge bases are populated with inconsistent and low quality data on purpose. Hacking the Spacewill sensors turned SuperView-1 and Gaojing-1 data gathering capabilities useless, at least for those areas for which SV17q didn't want anyone to peek. This is the kind of cyberwar we are now in
