The design of superintelligent artificial systems requires specifying decision rules based on utility functions.  The idea is then maximizing the expected utility for a given decision problem. On the other hand,  learning algorithms involve the gradual construction of some kind of evaluation function. But you cannot escape the fact that the  utility and the evaluation functions are always defined in terms of what a human understands by 'utility'. This means that in the long-term  the superintelligence will be superlearning from its exploration of both humans and their environment, to the point it  will become more and more humanlike in its decision making process. In the end, the ASI will become a mere human, posing no  real advantage for humans

An ASI that is always evil or always benevolent is a predictable entity, therefore a vulnerable one. Actually, the best ASI will be that showing a non-predictable behavior like humans show

They also designed ad deployed superintelligent artificial systems; and they also implemented an efficient confinement  method by simply having the ASI computer run a few clock cycles slower than a replica ASI system, this one designed to specifically  terminate the first ASI in case the crew took that decision

Contemporary  transhumanists  argue  that  human  nature  can  and  should  be  altered  through  technological  means where such enhancements are likely to lead to the majority of individuals living better lives. But they forget  to first solve the problem of who decide what is 'a better life'

Ultimately, the posthuman being is nothing if not the  same human being. With the modified and hybridized  body, with enhanced intellectual faculties and diluted  consciousness in space and time, with increased sensitiv- ity and no more diseases... but also with the same needs  and desires of human beings. Needs and desires that are post-human,  perhaps all too human

so superintelligent that it simulated its malfunctioning with the aim to analyze and learn what strategies the human programmers would put in place to remedy the simulated malfunction and confine it; fortunately, LyAv-OBASI was running 300 MHz slower than LyAv-Gemina, its mirror on-board artificial superintelligence, which was able to counter LyAv-OBASI efforts to terminate the human programmers

the ASI was tasked with the goal to fully analyze the dynamical characteristics of multi-body systems in order to identify transfer trajectories requiring very low energy input. It selected two solutions with an optimal mission length. Both proposed solutions turned to be wrong in just one paraneter: the length of the mission. After analysis by the programmers, they concluded the ASI dliberately chose those solutions that would look correct to humans, while reassuring the termination of the crew

Noise in the Nervous System
