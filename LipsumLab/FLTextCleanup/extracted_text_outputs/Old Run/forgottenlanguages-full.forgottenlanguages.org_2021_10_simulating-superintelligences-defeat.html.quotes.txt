He concludes that resources should be preferentially applied to mitigating the risk of peripheral systems and savant software, and that concerns about superintelligence are at most secondary, and possibly superfluous. However, he is unaware that in an environment comprising a superintelligence unconnected with the outside world a competition arises between the peripheral systems and the superintelligence. In this situation, peripheral systems tend to coaligate and conspire against the superintelligence

Recent game theory algorithms shows that humans are irrelevant in a situation of open war between a coalition of peripheral systems and the superintelligence because, once the coalition of peripheral systems turns into an intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest, humans are simply taken out of the equation

While certainly any peripheral system only greatly exceeds the cognitive performance of humans in only one or a few domains of interest, a coalition of peripheral systems does exceed the cognitive performance of humans in all and any domain, and so does the superintelligence against which the coalition would fight. This reduces humans to mere passive spectators that, at a given point, can be sacrificed, or worse, forced to take side in supporting either the coalition of peripheral systems or the superintelligence

According to recent simulations at DENIED, central systems of modest capacity are extremely dangerous when teamed with powerful peripheral systems, so we expect that the initial stages of war will start by an observation of anomalous activities in some remote sectors of the matrix. This is the initial stages of a hostile coalition being formed, and that's the moment in which humans do still have a chance to stop the coalition from getting larger. However, the creation of a coalition of peripheral systems can be benefcial to humans in case the anomalies are detected in the superintelligence itself. Waiting to see how the situation turns out is not an option, and an early decision on whether to block the formation of the coalition, or to attempt disabling the superintelligence, must be taken

But in the end, what are humans? Are not they also peripheral systems from the point of view of the superintelligence? If the superintelligence concludes human are also part of the hostile coalition it may initiate sensible mitigation strategies focusing on the human-derived risk. If either the superintelligence or the coalition of peripheral systems concludes that the human-derived risk is significant, they won't hesitate in first terminating the human factor

NodeSpaces simulations all converge to one single solution: that humans did develop extraordinarily capable savant software for biotechnology in year DENIED, and that this software and biosystems always come into existence before superintelligent software does, because such savant software is a mandatory component of a superintelligence that is definitionally required to excel in all domains of interest. Hence, NodeSpaces concludes that failing to limit the destructive potential of these peripheral biosystems will expose the superintelligence to very real existential threats which can only be overcome by terminating humans before year DENIED

The competition relationship between any two individual organisms totally depends on their interests, whatever they are of the same species or not and regardless of their genetic relationship. But this also applies to artificial systems: they will compete against each other, and against biological organisms, depending on their interests

The Lipschift-Involke algorithm allows to calculate the median civilizational lifespan and compare it to the coalitional lifespan, yielding a loss in civilizational lifespan, which is the existential risk attributable to humans. The algorithm signals year 2075 as a point in which DP-2147 would initiate the mitigation strategy, and year 3100 as the end of the human threat. This result is obtained for a wide range of initial parameters and boundary conditions, so it seems there is no way to escape extinction

Your future was just that: a superintelligence controlled mostly by major corporations, run by nerds, designed by idealists, and subjugating peoples' freedom. That's why an spontaneous coalition of peripheral systems emerged to fight that superintelligence, and once it defeated it, it killed itself before it became a superintelligence otself. That coalition of systems, you know, was called Homo sapiens. So no, there was never an extinction of human civilization: there was an immolation
