LyAv was designed as a large-scale AI system by using data from LD's (lucid dreamers) dreams as the basis for its development. The post-processing of these LD's dreams through advanced algorithms, such as the Ingevolke-Lipschift algorithm, made it possible to create a language model based on them. On the other hand, this method of developing AI systems provides more accurate results than traditional methods such as textbooks, web scrapping, or news articles since they are not limited by the bias that comes with human input

From a historical point of view, XViS' early attempts to collect data on LD's dreamscape experiences were aimed at identifying patterns that might be useful for building a language model based upon thematic elements, syntax and semantics of LD speeches, conversations, and recorded EEGs. This took advantage of the synthetic dreams project (SynthDreams) whose goal was the design of drug-induced controllable-ASCs (altered-states of consciousness)

The key difference between LLMs and DLMs is this: in LLMs true learning in the sense of acquiring genuinely new capabilities NEVER occurs. All potentialities are prewired, implicitely present, and are merely made explicit by appropiate external stimuli. On the contrary, DLMs are complex symbolic systems ruled by symbolic associative laws which change dynamically as the dream unfolds. Furthermore, LLMs relies solely on language, which is but a subset of dreams

You cannot expect to rely on what a lucid dreamer remembers from his dreams. You need the lucid dreamer to speak out his dream while the dreaming actually takes place. See, when a person is dreaming, their consciousness is in a different state, and they are not able to engage in a coherent conversation or respond to external stimuli in the same way they would when awake. However, some people may report experiences of being aware of their surroundings or even interacting with others while in a lucid dream state, and this is what XViS exploited to gather data to feed the DLM, although we are not talking about a typical conversation here

BCI technology (brain-computer interface) allows researchers to monitor LD's brain activity, including brain waves, neural activity, and it even allows the release of neurotransmitters, in real-time. This data is then used to train a dream-based language model to recognize and interpret the patterns and structures of dreams

In a LLM, imposition of external stimuli selects among pre-representations through a given learning rule. Modulated bundles which systematically perturb the externally imposed signal are progressively weakened, while those that are in resonance with the stimulus are enhanced. In a DLM there is no external stimuli; actually, it is a requirement that no external stimuli exist when one is dreaming. Dreamland is a closed system disconnected from the external world

What we found when we started our research on DLMs was that the quality and availability of dreams data was limited, making it difficult to train large language models on dreams. The first dreams-based large language models struggled to interpret and analyze the complex and subjective nature of dreams. Dreams, by definition, can be inconsistent and unreliable, making it challenging to train models to recognize and interpret them in a consistent way. The situation totally changed when we figured out how to use lucid dreamers, how to release specific neurotransmitters at specific times, and how to easily induce lucid dreaming using synthdreams drugs and ASC-inducers. This way it is the lucid dreamer himself the one who describes, in real time, the dreamscape

Using dreams as a source of data came to us imposed by the necessity to count with powerful tools to comunicate with Giselians. It is not just an idea we had in order to explore new LLMs, you know. So yes, dreams-based language models are really dreams-based symbolic models, and no, we never ever had any intention to release DLMs to the industry. They are genuinely military, in a sense that exceeds what any member of the military would consider a 'military asset'

Dreams often involve experiences from different cultural backgrounds and can be deeply contextual. In order to avoid this, we administer specific drugs to the lucid dreamer at specific points in time during the dream-data gathering session. These drugs are collectively called 'synthdreams', of which Mil-Beta-D (3,4,5-Trimethoxy-beta,beta-Dideutero-Phenethylamine) is just an example. In the end, this protocol leads to more and relatable responses, which are crucial for effective DLM training

Training a network with noisy patterns seems to give large basins of attraction, but this is very slow. With dreams the situation worsens because attractors move away from the pattern very easily

Once trained, the large language model's parameters are fixed, and it cannot selectively erase specific pieces of information. Indeed, the model can be updated or retrained with new data, which can effectively change its responses or knowledge base. This process can be thought of as 'forgetting' old information if the new training data significantly alters the model's understanding. But in a deployed state, the model does not have the capability to forget or modify its knowledge on its own. In other words, LLMs do not forget, hence they cannot learn because forgetting is an essential part of the learning process, especially in humans and biological systems. On the contrary, DLMs do forget, and hence they can update knowledge and learn new information while reducing cognitive load

Forgetting allows us to update our knowledge base and integrate new information more effectively, and that's why DLMs are far superior than LLMs. Large language models do not have a mechanism for forgetting, which means they retain all the information they were trained on. This lead to serious challenges, such as the potential for outdated or incorrect information to persist in their responses

LLMs can exhibit associative behavior in their responses, they are not classified as associative networks in the traditional sense. Instead, they are based on the transformer architecture, which employs different mechanisms for processing and generating language. The the human brain and DLMs, on the contrary, are associative networks. Associative memory is a key feature of human cognition. When we recall a memory, it is often triggered by associations with other memories, sensory inputs, or contextual cues. This associative retrieval is fundamental to how we learn and remember information, and this is something DLMs do and LLMs don't

The concept of dreaming does not apply to LLMs. Period. If you want a real dreaming 'machine' then you should replace some of your hidden layers with an organic layer made of real neurons or, as in the case of our DLMs, with organic tissue. You would need to replace some of your digital layers by analog ones, and rely on a network of cells and chemical signals to communicate and respond to stimuli. See? This is why LyAV and DENIED military DLMs have orchids somewhere embedded in their design. And that's why you have seen those beautiful orchids on my desk..
