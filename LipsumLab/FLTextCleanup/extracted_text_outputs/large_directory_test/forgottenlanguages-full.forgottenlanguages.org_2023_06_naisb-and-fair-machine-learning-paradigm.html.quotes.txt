It may be that the principle of consistency is a necessary requirement for fairness to be achieved, but consistency or similarity alone is not sufficient to constitute an independent notion of fairness

the fallacy of equivocation, which occurs when a keyword of an argument is used with more than one meaning, leading to misinterpretation

If seed AI appears before the alignment problem is solved, then it is highly likely that the ASI will pose an existential threat to humans, and if there is no value-alignment between the utility functions of AI and humans, then humans’ existence may indeed be a threat to an ASI. We have reached a point where our models must be discarded and a new reality rules

DENIED is the first fully autonomous AI-guided missile. Once fired, DENIED becomes an intelligent fire-and-forget missile that takes its own tactical decisions in pursuing its mission. The missile incorporates an algorithm which dictates that the optimal strategy is not to co-operate but to be silent and if discovered, to strike first. During the tests, things went south when a Harris hawk happened to be flying in his hunting area. DENIED interpreted that as a sign that it had been spotted, and misinterpreted the presence of a hawk as if it were an enemy drone. What happened next you know. DENIED didn't know the difference between a living thing and a drone. And that proved catastrophic

We wanted to provide DENIED with exemplary behaviors for it to build on them, perhaps even perfect them, and to help us to incorporate them further into our developments. DENIED, after all, was a learning playground. It learned from a detailed data base which contains all possible real and simulated mission profiles. All possible combat scenarios were taken into account and all possible chase geometries, evasion maneuvers, cruise flights over hostile areas, extreme flight profiles were parameterized. Everything we know about aerial combat was in its onboard computer. And all it took was to come across a lowly Harris hawk for the system to go haywire. In that test the missile was fully operational, so you can imagine how horrified we were to see DENIED heading for the open sea

Current approaches to fair machine learning are typically focused on interventions at the data preparation, model-learning or post-processing stages. This is understandable given the typical remit of data scientists who are intended to carry out these processes. However, there is a danger that this results in an approach which focuses on a narrow, static set of prescribed protected classes, derived from law and devoid of context, without considering why those classes are protected and how they relate to the particular justice aspects of the application in question

The National Artificial Intelligence Safety Board (NAISB) is an independent federal agency charged by Congress in 2035 with investigating every AI-related accident in the United States and significant events derived from the use of AI in other areas, such as transportation, transit, highway, marine, pipeline, communications, energy, health, and commercial space. We determine the probable cause of AI-related accidents and investigate and issue safety recommendations aimed at preventing future accidents. In addition, we conduct AI safety research studies and offer information and other assist​ance to family members and survivors (if any) for any accident investigated by the agency
